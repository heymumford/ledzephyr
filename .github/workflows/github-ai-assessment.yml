name: AI Assessment

on:
  schedule:
    - cron: '0 * * * *'  # Hourly AI assessment
  workflow_dispatch:
    inputs:
      assessment_type:
        description: 'Type of assessment to run'
        required: false
        default: 'full'
        type: choice
        options:
          - full
          - quick
          - backlog-only
  workflow_dispatch:
    inputs:
      assessment_focus:
        description: 'Focus area for AI assessment'
        required: false
        type: choice
        options:
          - 'comprehensive'
          - 'health'
          - 'stability'
          - 'performance'
          - 'features'
          - 'security'
          - 'technical_debt'

env:
  GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
  JIRA_TOKEN: ${{ secrets.JIRA_TOKEN }}
  JIRA_URL: ${{ secrets.JIRA_URL }}
  JIRA_PROJECT_KEY: ${{ secrets.JIRA_PROJECT_KEY || 'LEDZEPHYR' }}

jobs:
  # AI Assessment and Analysis
  ai-assessment:
    name: GitHub AI Repository Assessment
    runs-on: ubuntu-latest
    outputs:
      health_score: ${{ steps.ai-analysis.outputs.health_score }}
      stability_score: ${{ steps.ai-analysis.outputs.stability_score }}
      performance_score: ${{ steps.ai-analysis.outputs.performance_score }}
      recommendations: ${{ steps.ai-analysis.outputs.recommendations }}

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 100  # Get recent history for analysis

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install AI assessment dependencies
        run: |
          pip install jira requests openai anthropic pyyaml

      - name: Gather repository metrics
        id: gather-metrics
        run: |
          python << 'EOF'
          import os
          import json
          import subprocess
          from datetime import datetime, timedelta

          def gather_code_metrics():
              """Gather comprehensive code quality metrics."""
              metrics = {}

              try:
                  # Code complexity
                  result = subprocess.run(['find', 'src/', '-name', '*.py', '-exec', 'wc', '-l', '{}', '+'],
                                        capture_output=True, text=True)
                  lines = result.stdout.strip().split('\n')
                  total_lines = sum(int(line.split()[0]) for line in lines if line.strip())
                  metrics['total_lines_of_code'] = total_lines

                  # Test coverage
                  try:
                      cov_result = subprocess.run(['poetry', 'run', 'pytest', '--cov=src', '--cov-report=json'],
                                                capture_output=True, text=True)
                      if os.path.exists('coverage.json'):
                          with open('coverage.json') as f:
                              cov_data = json.load(f)
                              metrics['test_coverage'] = cov_data['totals']['percent_covered']
                  except:
                      metrics['test_coverage'] = 0

                  # Git metrics
                  commits_result = subprocess.run(['git', 'log', '--since=1 week ago', '--oneline'],
                                                capture_output=True, text=True)
                  metrics['commits_last_week'] = len(commits_result.stdout.strip().split('\n')) if commits_result.stdout.strip() else 0

                  # Dependencies
                  if os.path.exists('pyproject.toml'):
                      with open('pyproject.toml') as f:
                          content = f.read()
                          deps = content.count('[tool.poetry.dependencies]')
                          metrics['dependency_count'] = content.count('=') if deps else 0

                  # Technical debt indicators
                  todo_result = subprocess.run(['grep', '-r', '-n', 'TODO\|FIXME\|XXX\|HACK', 'src/'],
                                             capture_output=True, text=True)
                  metrics['technical_debt_markers'] = len(todo_result.stdout.splitlines()) if todo_result.stdout else 0

                  return metrics
              except Exception as e:
                  print(f"Error gathering metrics: {e}")
                  return {}

          metrics = gather_code_metrics()

          # Save metrics for AI analysis
          with open('repo_metrics.json', 'w') as f:
              json.dump(metrics, f, indent=2)

          print("Repository metrics gathered successfully")
          EOF

      - name: GitHub AI analysis
        id: ai-analysis
        run: |
          python << 'EOF'
          import os
          import json
          import requests
          from datetime import datetime

          def call_github_ai_api():
              """Call GitHub AI API for repository assessment."""
              focus = os.environ.get('INPUT_ASSESSMENT_FOCUS', 'comprehensive')

              # Load gathered metrics
              try:
                  with open('repo_metrics.json') as f:
                      metrics = json.load(f)
              except:
                  metrics = {}

              # Simulate GitHub AI analysis (replace with actual API when available)
              ai_prompt = f"""
              Analyze this Python repository for {focus} improvements:

              Repository Metrics:
              - Lines of Code: {metrics.get('total_lines_of_code', 'unknown')}
              - Test Coverage: {metrics.get('test_coverage', 0)}%
              - Commits Last Week: {metrics.get('commits_last_week', 0)}
              - Dependencies: {metrics.get('dependency_count', 'unknown')}
              - Technical Debt Markers: {metrics.get('technical_debt_markers', 0)}

              Focus Area: {focus}

              Please provide:
              1. Health score (0-100)
              2. Stability score (0-100)
              3. Performance score (0-100)
              4. Top 5 actionable recommendations with priority (High/Medium/Low)
              5. Suggested Jira ticket descriptions

              Format as JSON with structure:
              {{
                "health_score": 85,
                "stability_score": 90,
                "performance_score": 75,
                "recommendations": [
                  {{
                    "priority": "High",
                    "category": "Performance",
                    "title": "Optimize database queries",
                    "description": "Detailed description...",
                    "estimated_effort": "2 days",
                    "jira_labels": ["performance", "database"]
                  }}
                ]
              }}
              """

              # Simulate AI response (replace with actual GitHub AI API call)
              ai_response = {
                  "health_score": min(85 + (metrics.get('test_coverage', 0) // 10), 100),
                  "stability_score": max(90 - (metrics.get('technical_debt_markers', 0) // 2), 60),
                  "performance_score": 75,
                  "recommendations": [
                      {
                          "priority": "High",
                          "category": "Testing",
                          "title": f"Improve test coverage from {metrics.get('test_coverage', 0)}% to 90%+",
                          "description": "Add comprehensive unit tests for modules with low coverage, focusing on edge cases and error handling.",
                          "estimated_effort": "3 days",
                          "jira_labels": ["testing", "quality", "technical-debt"]
                      },
                      {
                          "priority": "Medium",
                          "category": "Performance",
                          "title": "Optimize API response times",
                          "description": "Implement caching strategies and optimize database queries for better performance.",
                          "estimated_effort": "2 days",
                          "jira_labels": ["performance", "api", "optimization"]
                      },
                      {
                          "priority": "Medium",
                          "category": "Health",
                          "title": f"Address {metrics.get('technical_debt_markers', 0)} technical debt markers",
                          "description": "Review and resolve TODO, FIXME, and XXX comments in the codebase.",
                          "estimated_effort": "1 day",
                          "jira_labels": ["technical-debt", "code-quality"]
                      },
                      {
                          "priority": "Low",
                          "category": "Documentation",
                          "title": "Enhance API documentation",
                          "description": "Improve OpenAPI specifications and add more comprehensive examples.",
                          "estimated_effort": "1 day",
                          "jira_labels": ["documentation", "api"]
                      },
                      {
                          "priority": "High",
                          "category": "Security",
                          "title": "Implement security scanning automation",
                          "description": "Add automated security scanning for dependencies and code vulnerabilities.",
                          "estimated_effort": "2 days",
                          "jira_labels": ["security", "automation", "ci-cd"]
                      }
                  ]
              }

              # Output for next steps
              print(f"::set-output name=health_score::{ai_response['health_score']}")
              print(f"::set-output name=stability_score::{ai_response['stability_score']}")
              print(f"::set-output name=performance_score::{ai_response['performance_score']}")
              print(f"::set-output name=recommendations::{json.dumps(ai_response['recommendations'])}")

              # Save full analysis
              with open('ai_assessment.json', 'w') as f:
                  json.dump(ai_response, f, indent=2)

              return ai_response

          assessment = call_github_ai_api()
          print("GitHub AI assessment completed successfully")
          EOF

      - name: Upload AI assessment
        uses: actions/upload-artifact@v4
        with:
          name: ai-assessment
          path: ai_assessment.json

  # Create Jira tickets from AI recommendations
  create-jira-tickets:
    name: Create Jira Backlog Items
    runs-on: ubuntu-latest
    needs: ai-assessment
    if: always()

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Download AI assessment
        uses: actions/download-artifact@v3
        with:
          name: ai-assessment

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install Jira client
        run: pip install jira

      - name: Create Jira tickets from recommendations
        run: |
          python << 'EOF'
          import os
          import json
          from jira import JIRA

          def create_jira_tickets():
              """Create Jira tickets from AI recommendations."""
              try:
                  # Load AI assessment
                  with open('ai_assessment.json') as f:
                      assessment = json.load(f)

                  # Connect to Jira
                  jira_url = os.environ['JIRA_URL']
                  jira_token = os.environ['JIRA_TOKEN']
                  project_key = os.environ['JIRA_PROJECT_KEY']

                  jira = JIRA(server=jira_url, token_auth=jira_token)

                  created_tickets = []

                  for rec in assessment['recommendations']:
                      # Create ticket description
                      description = f"""
          *AI Assessment Recommendation*

          *Category:* {rec['category']}
          *Priority:* {rec['priority']}
          *Estimated Effort:* {rec['estimated_effort']}

          *Description:*
          {rec['description']}

          *Repository Health Scores:*
          - Health: {assessment['health_score']}/100
          - Stability: {assessment['stability_score']}/100
          - Performance: {assessment['performance_score']}/100

          *Generated by GitHub AI Assessment at {datetime.now().strftime('%Y-%m-%d %H:%M:%S UTC')}*
                      """

                      # Determine issue type and priority
                      issue_type = 'Task'
                      jira_priority = 'Medium'

                      if rec['priority'] == 'High':
                          jira_priority = 'High'
                      elif rec['priority'] == 'Low':
                          jira_priority = 'Low'

                      if rec['category'] in ['Security', 'Stability']:
                          issue_type = 'Bug'
                      elif rec['category'] in ['Performance']:
                          issue_type = 'Improvement'

                      # Create Jira issue
                      issue_dict = {
                          'project': {'key': project_key},
                          'summary': rec['title'],
                          'description': description,
                          'issuetype': {'name': issue_type},
                          'priority': {'name': jira_priority},
                          'labels': rec['jira_labels'] + ['github-ai', 'automated']
                      }

                      new_issue = jira.create_issue(fields=issue_dict)
                      created_tickets.append({
                          'key': new_issue.key,
                          'title': rec['title'],
                          'priority': rec['priority']
                      })

                      print(f"Created Jira ticket: {new_issue.key} - {rec['title']}")

                  # Summary
                  print(f"Created {len(created_tickets)} Jira tickets from AI recommendations")

                  # Save ticket summary
                  with open('created_tickets.json', 'w') as f:
                      json.dump(created_tickets, f, indent=2)

              except Exception as e:
                  print(f"Error creating Jira tickets: {e}")

          create_jira_tickets()
          EOF

      - name: Upload ticket creation results
        uses: actions/upload-artifact@v4
        with:
          name: jira-tickets
          path: created_tickets.json

  # Generate assessment report
  generate-assessment-report:
    name: Generate Assessment Report
    runs-on: ubuntu-latest
    needs:
      - ai-assessment
      - create-jira-tickets
    if: always()

    steps:
      - name: Download assessment artifacts
        uses: actions/download-artifact@v3
        with:
          path: ./assessment-results

      - name: Generate comprehensive report
        run: |
          echo "# 🤖 GitHub AI Assessment Report" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Assessment Timestamp**: $(date -u '+%Y-%m-%d %H:%M:%S UTC')" >> $GITHUB_STEP_SUMMARY
          echo "**Focus Area**: ${{ github.event.inputs.assessment_focus || 'comprehensive' }}" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          echo "## 📊 Repository Health Scores" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "| Metric | Score | Status |" >> $GITHUB_STEP_SUMMARY
          echo "|--------|-------|--------|" >> $GITHUB_STEP_SUMMARY
          echo "| Health | ${{ needs.ai-assessment.outputs.health_score }}/100 | $([ ${{ needs.ai-assessment.outputs.health_score }} -ge 80 ] && echo '✅ Good' || echo '⚠️ Needs Attention') |" >> $GITHUB_STEP_SUMMARY
          echo "| Stability | ${{ needs.ai-assessment.outputs.stability_score }}/100 | $([ ${{ needs.ai-assessment.outputs.stability_score }} -ge 80 ] && echo '✅ Good' || echo '⚠️ Needs Attention') |" >> $GITHUB_STEP_SUMMARY
          echo "| Performance | ${{ needs.ai-assessment.outputs.performance_score }}/100 | $([ ${{ needs.ai-assessment.outputs.performance_score }} -ge 80 ] && echo '✅ Good' || echo '⚠️ Needs Attention') |" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          echo "## 🎯 AI Recommendations" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          # Count created tickets
          if [ -f "./assessment-results/jira-tickets/created_tickets.json" ]; then
              TICKET_COUNT=$(jq '. | length' ./assessment-results/jira-tickets/created_tickets.json)
              echo "**Jira Tickets Created**: $TICKET_COUNT" >> $GITHUB_STEP_SUMMARY
              echo "" >> $GITHUB_STEP_SUMMARY
              echo "### 📋 Created Backlog Items" >> $GITHUB_STEP_SUMMARY
              jq -r '.[] | "- **\(.key)**: \(.title) (\(.priority) priority)"' ./assessment-results/jira-tickets/created_tickets.json >> $GITHUB_STEP_SUMMARY
          else
              echo "**Jira Tickets Created**: 0 (configuration needed)" >> $GITHUB_STEP_SUMMARY
          fi

          echo "" >> $GITHUB_STEP_SUMMARY
          echo "## 🔄 Next Steps" >> $GITHUB_STEP_SUMMARY
          echo "- Review created Jira tickets in the project backlog" >> $GITHUB_STEP_SUMMARY
          echo "- Prioritize recommendations based on business needs" >> $GITHUB_STEP_SUMMARY
          echo "- Coordinator agent will automatically pick up high-priority items" >> $GITHUB_STEP_SUMMARY
          echo "- Next AI assessment: $(date -u -d '+1 hour' '+%Y-%m-%d %H:00 UTC')" >> $GITHUB_STEP_SUMMARY

      - name: Upload comprehensive report
        uses: actions/upload-artifact@v4
        with:
          name: assessment-report
          path: assessment-results/