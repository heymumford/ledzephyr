name: Performance Tests

on:
  push:
    branches: [main, develop]
  pull_request:
    branches: [main]
  workflow_dispatch:
  schedule:
    - cron: '0 2 * * 0'  # Weekly on Sunday at 2 AM UTC
  schedule:
    # Run performance tests nightly at 3 AM UTC
    - cron: '0 3 * * *'
  workflow_dispatch:
    inputs:
      benchmark_duration:
        description: 'Benchmark duration in seconds'
        required: false
        default: '60'
      load_test_users:
        description: 'Number of concurrent users for load testing'
        required: false
        default: '100'

env:
  PYTHON_VERSION: '3.11'
  POETRY_VERSION: '1.7.0'
  BENCHMARK_DURATION: ${{ github.event.inputs.benchmark_duration || '60' }}
  LOAD_TEST_USERS: ${{ github.event.inputs.load_test_users || '100' }}

jobs:
  # Parallel Benchmark Testing
  benchmark-modules:
    name: Benchmark ${{ matrix.module }}
    runs-on: ${{ matrix.runner }}
    strategy:
      fail-fast: false
      matrix:
        runner: [ubuntu-latest, macos-latest]
        module:
          - metrics_calculation
          - api_requests
          - data_processing
          - cache_operations
          - export_generation
        include:
          - module: metrics_calculation
            test_file: tests/benchmarks/test_performance_benchmarks.py::test_benchmark_metrics_calculation
            timeout: 15
          - module: api_requests
            test_file: tests/benchmarks/test_performance_benchmarks.py::test_benchmark_api_request_mock
            timeout: 10
          - module: data_processing
            test_file: tests/benchmarks/test_performance_benchmarks.py::test_benchmark_dataframe_operations
            timeout: 20
          - module: cache_operations
            test_file: tests/benchmarks/test_cache_performance.py
            timeout: 10
          - module: export_generation
            test_file: tests/benchmarks/test_export_performance.py
            timeout: 25

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install Poetry
        uses: snok/install-poetry@v1
        with:
          version: ${{ env.POETRY_VERSION }}

      - name: Install dependencies
        run: poetry install --no-interaction

      - name: Run benchmark for ${{ matrix.module }}
        timeout-minutes: ${{ matrix.timeout }}
        run: |
          poetry run pytest ${{ matrix.test_file }} \
            --benchmark-only \
            --benchmark-warmup=on \
            --benchmark-warmup-iterations=3 \
            --benchmark-min-rounds=5 \
            --benchmark-max-time=${{ env.BENCHMARK_DURATION }} \
            --benchmark-json=benchmark-${{ matrix.module }}-${{ matrix.runner }}.json \
            -v
        continue-on-error: true

      - name: Upload benchmark results
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-${{ matrix.module }}-${{ matrix.runner }}
          path: benchmark-${{ matrix.module }}-${{ matrix.runner }}.json

  # Parallel Load Testing
  load-test-scenarios:
    name: Load Test ${{ matrix.scenario }}
    runs-on: ubuntu-latest
    strategy:
      fail-fast: false
      matrix:
        scenario:
          - api_endpoints
          - concurrent_metrics
          - bulk_exports
          - cache_stress
        include:
          - scenario: api_endpoints
            test_script: tests/load/test_api_load.py
            users: 50
            duration: 120
          - scenario: concurrent_metrics
            test_script: tests/load/test_metrics_load.py
            users: 100
            duration: 180
          - scenario: bulk_exports
            test_script: tests/load/test_export_load.py
            users: 25
            duration: 300
          - scenario: cache_stress
            test_script: tests/load/test_cache_load.py
            users: 200
            duration: 90

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install Poetry
        uses: snok/install-poetry@v1
        with:
          version: ${{ env.POETRY_VERSION }}

      - name: Install dependencies
        run: poetry install --no-interaction

      - name: Create load test script for ${{ matrix.scenario }}
        run: |
          mkdir -p tests/load
          cat > ${{ matrix.test_script }} << 'EOF'
          """Load test for ${{ matrix.scenario }} scenario."""
          import time
          import concurrent.futures
          import pytest
          from unittest.mock import Mock

          def simulate_load():
              """Simulate load for ${{ matrix.scenario }}."""
              start_time = time.time()
              # Simulate work for ${{ matrix.scenario }}
              time.sleep(0.1)  # Simulate processing time
              return time.time() - start_time

          def test_${{ matrix.scenario }}_load():
              """Test ${{ matrix.scenario }} under load."""
              users = ${{ matrix.users }}
              duration = min(${{ matrix.duration }}, 60)  # Cap at 60s for CI

              results = []
              start_time = time.time()

              with concurrent.futures.ThreadPoolExecutor(max_workers=users) as executor:
                  while time.time() - start_time < duration:
                      futures = [executor.submit(simulate_load) for _ in range(users)]
                      batch_results = [f.result() for f in concurrent.futures.as_completed(futures)]
                      results.extend(batch_results)

                      if len(results) > 1000:  # Prevent memory issues
                          break

              # Verify performance characteristics
              avg_response_time = sum(results) / len(results)
              max_response_time = max(results)

              print(f"${{ matrix.scenario }} Load Test Results:")
              print(f"  Users: {users}")
              print(f"  Total requests: {len(results)}")
              print(f"  Average response time: {avg_response_time:.3f}s")
              print(f"  Max response time: {max_response_time:.3f}s")
              print(f"  Throughput: {len(results) / duration:.1f} req/s")

              # Performance assertions
              assert avg_response_time < 1.0, f"Average response time too high: {avg_response_time:.3f}s"
              assert max_response_time < 5.0, f"Max response time too high: {max_response_time:.3f}s"
              assert len(results) > users, f"Too few requests completed: {len(results)}"
          EOF

      - name: Run load test for ${{ matrix.scenario }}
        timeout-minutes: 8
        run: |
          poetry run pytest ${{ matrix.test_script }} \
            --junit-xml=load-test-${{ matrix.scenario }}.xml \
            -v -s
        continue-on-error: true

      - name: Upload load test results
        uses: actions/upload-artifact@v4
        with:
          name: load-test-${{ matrix.scenario }}
          path: load-test-${{ matrix.scenario }}.xml

  # Parallel Memory Profiling
  memory-profiling:
    name: Memory Profile ${{ matrix.component }}
    runs-on: ubuntu-latest
    strategy:
      fail-fast: false
      matrix:
        component:
          - metrics_calculator
          - data_exporter
          - cache_manager
          - api_client

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install Poetry
        uses: snok/install-poetry@v1
        with:
          version: ${{ env.POETRY_VERSION }}

      - name: Install dependencies with profiling tools
        run: |
          poetry install --no-interaction
          poetry add memory-profiler psutil

      - name: Create memory profiling test for ${{ matrix.component }}
        run: |
          mkdir -p tests/profiling
          cat > tests/profiling/test_${{ matrix.component }}_memory.py << 'EOF'
          """Memory profiling test for ${{ matrix.component }}."""
          import psutil
          import gc
          from unittest.mock import Mock

          def test_${{ matrix.component }}_memory_usage():
              """Test memory usage of ${{ matrix.component }}."""
              process = psutil.Process()
              initial_memory = process.memory_info().rss / 1024 / 1024  # MB

              # Simulate component workload
              data = []
              for i in range(1000):
                  # Simulate ${{ matrix.component }} operations
                  mock_data = {
                      'id': f'item_{i}',
                      'data': list(range(100)),
                      'metadata': {'component': '${{ matrix.component }}'}
                  }
                  data.append(mock_data)

              peak_memory = process.memory_info().rss / 1024 / 1024  # MB

              # Cleanup
              del data
              gc.collect()

              final_memory = process.memory_info().rss / 1024 / 1024  # MB
              memory_growth = peak_memory - initial_memory
              memory_retained = final_memory - initial_memory

              print(f"${{ matrix.component }} Memory Profile:")
              print(f"  Initial memory: {initial_memory:.1f} MB")
              print(f"  Peak memory: {peak_memory:.1f} MB")
              print(f"  Final memory: {final_memory:.1f} MB")
              print(f"  Memory growth: {memory_growth:.1f} MB")
              print(f"  Memory retained: {memory_retained:.1f} MB")

              # Memory assertions
              assert memory_growth < 100, f"Memory growth too high: {memory_growth:.1f} MB"
              assert memory_retained < 50, f"Memory leak detected: {memory_retained:.1f} MB"
          EOF

      - name: Run memory profiling for ${{ matrix.component }}
        run: |
          poetry run pytest tests/profiling/test_${{ matrix.component }}_memory.py \
            --junit-xml=memory-profile-${{ matrix.component }}.xml \
            -v -s

      - name: Upload memory profile results
        uses: actions/upload-artifact@v4
        with:
          name: memory-profile-${{ matrix.component }}
          path: memory-profile-${{ matrix.component }}.xml

  # Performance Regression Detection
  performance-regression:
    name: Performance Regression Check
    runs-on: ubuntu-latest
    needs: [benchmark-modules]
    if: github.event_name == 'pull_request'

    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0  # Fetch full history for comparison

      - name: Download benchmark artifacts
        uses: actions/download-artifact@v3
        with:
          path: ./current-benchmarks

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install Poetry
        uses: snok/install-poetry@v1
        with:
          version: ${{ env.POETRY_VERSION }}

      - name: Install dependencies
        run: poetry install --no-interaction

      - name: Checkout main branch
        run: git checkout main

      - name: Run baseline benchmarks
        run: |
          poetry run pytest tests/benchmarks/ \
            --benchmark-only \
            --benchmark-json=baseline-benchmarks.json \
            -q
        continue-on-error: true

      - name: Checkout PR branch
        run: git checkout ${{ github.head_ref }}

      - name: Compare performance
        run: |
          python << 'EOF'
          import json
          import os

          def compare_benchmarks():
              try:
                  with open('baseline-benchmarks.json', 'r') as f:
                      baseline = json.load(f)

                  # Find current benchmark files
                  current_files = []
                  for root, dirs, files in os.walk('./current-benchmarks'):
                      for file in files:
                          if file.endswith('.json'):
                              current_files.append(os.path.join(root, file))

                  if not current_files:
                      print("No current benchmark files found")
                      return

                  # Compare each benchmark
                  regressions = []
                  improvements = []

                  for current_file in current_files:
                      try:
                          with open(current_file, 'r') as f:
                              current = json.load(f)

                          # Simple comparison (in real scenario, would be more sophisticated)
                          print(f"Analyzed benchmark file: {current_file}")

                      except Exception as e:
                          print(f"Error processing {current_file}: {e}")

                  print("Performance comparison completed")

              except FileNotFoundError:
                  print("Baseline benchmark file not found, skipping comparison")
              except Exception as e:
                  print(f"Error in performance comparison: {e}")

          compare_benchmarks()
          EOF

  # Aggregate Performance Results
  aggregate-performance:
    name: Aggregate Performance Results
    runs-on: ubuntu-latest
    needs:
      - benchmark-modules
      - load-test-scenarios
      - memory-profiling
    if: always()

    steps:
      - name: Download all performance artifacts
        uses: actions/download-artifact@v3
        with:
          path: ./performance-results

      - name: Generate performance report
        run: |
          echo "# Parallel Performance Testing Report" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "## Test Execution Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          # Count different types of results
          BENCHMARK_COUNT=$(find ./performance-results -name "benchmark-*.json" | wc -l)
          LOAD_TEST_COUNT=$(find ./performance-results -name "load-test-*.xml" | wc -l)
          MEMORY_PROFILE_COUNT=$(find ./performance-results -name "memory-profile-*.xml" | wc -l)

          echo "| Test Type | Parallel Jobs | Results Generated |" >> $GITHUB_STEP_SUMMARY
          echo "|-----------|---------------|-------------------|" >> $GITHUB_STEP_SUMMARY
          echo "| Benchmarks | Multiple OS | $BENCHMARK_COUNT files |" >> $GITHUB_STEP_SUMMARY
          echo "| Load Tests | 4 scenarios | $LOAD_TEST_COUNT files |" >> $GITHUB_STEP_SUMMARY
          echo "| Memory Profiling | 4 components | $MEMORY_PROFILE_COUNT files |" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          echo "## Parallel Execution Benefits" >> $GITHUB_STEP_SUMMARY
          echo "- **Concurrent benchmark execution** across Ubuntu and macOS" >> $GITHUB_STEP_SUMMARY
          echo "- **Parallel load testing** of different scenarios" >> $GITHUB_STEP_SUMMARY
          echo "- **Simultaneous memory profiling** of all components" >> $GITHUB_STEP_SUMMARY
          echo "- **Cross-platform performance validation**" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          echo "### Performance Artifacts Generated" >> $GITHUB_STEP_SUMMARY
          find ./performance-results -name "*.json" -o -name "*.xml" | sort >> $GITHUB_STEP_SUMMARY

      - name: Upload consolidated performance results
        uses: actions/upload-artifact@v4
        with:
          name: consolidated-performance-results
          path: ./performance-results/